{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "eps = np.finfo(float).eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weight_init(nd, init_mode='random'):\n",
    "\tif init_mode == 'random':\n",
    "\t\ttheta = np.random.randn(nd + 1, 1) # plus the bias/const term\n",
    "\telif init_mode=='xavier':\n",
    "\t\t\"\"\"\n",
    "\t\tGlorot, Xavier, and Yoshua Bengio. \n",
    "\t\t\"Understanding the difficulty of training deep feedforward neural networks.\" \n",
    "\t\tProceedings of the thirteenth international conference on artificial \n",
    "\t\tintelligence and statistics. 2010.\n",
    "\t\t\"\"\"\n",
    "\t\tpass\n",
    "\telse:\n",
    "\t\tpass\n",
    "\treturn theta\n",
    "\n",
    "def cross_entropy_loss(pred, y):\n",
    "\t\"\"\"\n",
    "\thttps://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_loss_function_and_logistic_regression\n",
    "\t\"\"\"\n",
    "\t# n = y.shape[0]\n",
    "\t# loss = -(y*np.log(pred+eps) + (1-y)*np.log(1-pred+eps))/n\n",
    "\t# loss = loss.sum(axis=0)\n",
    "\t# return loss\n",
    "\t\"\"\"\n",
    "\tfor numerical stability use log sum:\n",
    "\t\t??max_val = (-input).clamp(min=0)\n",
    "\t\t??loss = input - input * target + max_val + ((-max_val).exp() + (-input - max_val).exp()).log()\n",
    "\t\thttps://discuss.pytorch.org/t/numerical-stability-of-bcewithlogitsloss/8246\n",
    "\t\thttp://tagkopouloslab.ucdavis.edu/?p=2197\n",
    "\t\thttps://www.xarg.org/2016/06/the-log-sum-exp-trick-in-machine-learning/\n",
    "\t\"\"\"\n",
    "\tn = y.shape[0]\n",
    "\tmax_value = np.clip(pred, eps, 1-eps)\n",
    "\tloss = -(y*np.log(max_value) + (1-y)*np.log(1-max_value))/n\n",
    "\tloss = loss.sum(axis=0)\n",
    "\treturn loss\n",
    "\n",
    "def _pos_sigmoid(x):\n",
    "\tz = np.exp(-x)\n",
    "\treturn 1 / (1 + z)\n",
    "\n",
    "def _neg_sigmoid(x):\n",
    "\tz = np.exp(x)\n",
    "\treturn z / (1 + z)\n",
    "\n",
    "def sigmoid(x):\n",
    "\t#return 1.0 / (1.0 + np.exp(-x))\n",
    "\t\"\"\"\n",
    "\tNumerically stable sigmoid function.\n",
    "\t\tsee: https://timvieira.github.io/blog/post/2014/02/11/exp-normalize-trick/\n",
    "\t\"\"\"\n",
    "\tsx = x.copy()\n",
    "\tpos_ind = sx >= 0\n",
    "\tneg_ind = ~pos_ind\n",
    "\n",
    "\tsx[pos_ind] = _pos_sigmoid(sx[pos_ind])\n",
    "\tsx[neg_ind] = _neg_sigmoid(sx[neg_ind])\n",
    "\treturn sx\n",
    "\n",
    "def compute_accuracy(X, yin, theta):\n",
    "\ty = yin.copy()\n",
    "\tn = y.shape[0]\n",
    "\tgz = sigmoid(X @ theta)\n",
    "\tacc = (gz == y).sum()/n\n",
    "\treturn acc*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X, y = breast_cancer.data, breast_cancer.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ni, nd = X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 1e-3\n",
    "bsize = 569\n",
    "max_epochs = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.append(X, np.ones((ni, 1)), axis=1) # append the bias/const term\n",
    "theta = weight_init(nd, init_mode='random')\n",
    "y = np.expand_dims(y, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "599785.303706\n"
     ]
    }
   ],
   "source": [
    "ind = 0\n",
    "loss = 0\n",
    "while ind < ni:\n",
    "    end = ind + bsize if ind + bsize <= ni else ni\n",
    "    bX = X[ind:end]\n",
    "    by = y[ind:end]\n",
    "    gz = sigmoid(np.matmul(bX, theta))\n",
    "    pred = gz\n",
    "    loss += cross_entropy_loss(pred, by)\n",
    "    grad = (np.matmul(bX.T, (pred - by))).sum(axis=1)\n",
    "    grad = np.expand_dims(grad, 1)\n",
    "    print(grad.sum())\n",
    "    theta -= lr * grad  # sgd\n",
    "    ind += bsize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = compute_accuracy(X, y, theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "62.741652021089635"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "yy = np.expand_dims(y, axis=1)\n",
    "gz = sigmoid(X @ theta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62741652]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(yy)/ni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = LogisticRegression(C=1.0, penalty='none',\n",
    "                        solver='sag',\n",
    "                        multi_class='multinomial',\n",
    "                        max_iter=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/peratham/Programs/kaggleconda3/lib/python3.8/site-packages/sklearn/utils/validation.py:72: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  return f(**kwargs)\n",
      "/home/peratham/Programs/kaggleconda3/lib/python3.8/site-packages/sklearn/linear_model/_sag.py:329: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\"The max_iter was reached which means \"\n"
     ]
    }
   ],
   "source": [
    "classifier.fit(X, y)\n",
    "y_pred = classifier.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
       "       1, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1,\n",
       "       1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1,\n",
       "       0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 0, 0, 1, 1,\n",
       "       1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
       "       1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.05314414027631"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y == y_pred).mean()*100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
